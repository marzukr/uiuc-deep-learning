{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "hGDh3heHW-i9",
    "outputId": "91ddaeba-88cf-4ac4-84f5-2d5b93bfa0ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "qSaSSf1SXP4S",
    "outputId": "8fa941c1-f049-4299-9e1c-92c12066ad19"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/My Drive\n",
      "/content/drive/My Drive/cs498 DL/Assignment4\n"
     ]
    }
   ],
   "source": [
    "%cd /content/drive/My\\ Drive/\n",
    "%cd cs498\\ DL/Assignment4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o_iMajjbayH_",
    "outputId": "ba2b8636-b4d8-46e8-f739-7f6cf55eec68"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Unidecode in /usr/local/lib/python3.6/dist-packages (1.1.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install Unidecode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hJ7A-AbBKa1s"
   },
   "source": [
    "# Generating Text with an RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ctTbJ9EpKa1s"
   },
   "outputs": [],
   "source": [
    "import unidecode\n",
    "import string\n",
    "import random\n",
    "import re\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TX1n1aSqKa1t"
   },
   "outputs": [],
   "source": [
    "from rnn.model import RNN\n",
    "from rnn.helpers import time_since\n",
    "from rnn.generate import generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mlhJV8sOKa1t",
    "outputId": "41bf7953-0c38-40fd-c037-1f8a40401f41",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") \n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "psv4DqszKa1t"
   },
   "source": [
    "## Data Processing\n",
    "\n",
    "The file we are using is a plain text file. We turn any potential unicode characters into plain ASCII by using the `unidecode` package (which you can install via `pip` or `conda`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "16x_ntAoKa1t",
    "outputId": "902b4a81-7c24-4032-a137-64f4d43bec9d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file_len = 4573338\n",
      "train len:  4116004\n",
      "test len:  457334\n"
     ]
    }
   ],
   "source": [
    "all_characters = string.printable\n",
    "n_characters = len(all_characters)\n",
    "\n",
    "file_path = './shakespeare.txt'\n",
    "file = unidecode.unidecode(open(file_path).read())\n",
    "file_len = len(file)\n",
    "print('file_len =', file_len)\n",
    "\n",
    "# we will leave the last 1/10th of text as test\n",
    "split = int(0.9*file_len)\n",
    "train_text = file[:split]\n",
    "test_text = file[split:]\n",
    "\n",
    "print('train len: ', len(train_text))\n",
    "print('test len: ', len(test_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zlXIvUObKa1t",
    "outputId": "1ad2834d-6146-406e-ee20-ed7785fd70f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is night to Messina.\n",
      "\n",
      "Messenger:\n",
      "He is very near by this: he was not three leagues off\n",
      "when I left him.\n",
      "\n",
      "LEONATO:\n",
      "How many gentlemen have you lost in this action?\n",
      "\n",
      "Messenger:\n",
      "But few of any sort, and n\n"
     ]
    }
   ],
   "source": [
    "chunk_len = 200\n",
    "\n",
    "def random_chunk(text):\n",
    "    start_index = random.randint(0, len(text) - chunk_len)\n",
    "    end_index = start_index + chunk_len + 1\n",
    "    return text[start_index:end_index]\n",
    "\n",
    "print(random_chunk(train_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2RuPsfuDKa1t"
   },
   "source": [
    "### Input and Target data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5IaWymCLKa1t"
   },
   "source": [
    "To make training samples out of the large string of text data, we will be splitting the text into chunks.\n",
    "\n",
    "Each chunk will be turned into a tensor, specifically a `LongTensor` (used for integer values), by looping through the characters of the string and looking up the index of each character in `all_characters`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5MRQnyomKa1t"
   },
   "outputs": [],
   "source": [
    "# Turn string into list of longs\n",
    "def char_tensor(string):\n",
    "    tensor = torch.zeros(len(string), requires_grad=True).long()\n",
    "    for c in range(len(string)):\n",
    "        tensor[c] = all_characters.index(string[c])\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C-JloYDgKa1t"
   },
   "source": [
    "The following function loads a batch of input and target tensors for training. Each sample comes from a random chunk of text. A sample input will consist of all characters *except the last*, while the target wil contain all characters *following the first*. For example: if random_chunk='abc', then input='ab' and target='bc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "56UiWvpzKa1t"
   },
   "outputs": [],
   "source": [
    "def load_random_batch(text, chunk_len, batch_size):\n",
    "    input_data = torch.zeros(batch_size, chunk_len).long().to(device)\n",
    "    target = torch.zeros(batch_size, chunk_len).long().to(device)\n",
    "    for i in range(batch_size):\n",
    "        start_index = random.randint(0, len(text) - chunk_len - 1)\n",
    "        end_index = start_index + chunk_len + 1\n",
    "        chunk = text[start_index:end_index]\n",
    "        input_data[i] = char_tensor(chunk[:-1])\n",
    "        target[i] = char_tensor(chunk[1:])\n",
    "    return input_data, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y9vH6KWiKa1t"
   },
   "source": [
    "# Implement model\n",
    "\n",
    "Your RNN model will take as input the character for step $t_{-1}$ and output a prediction for the next character $t$. The model should consiste of three layers - a linear layer that encodes the input character into an embedded state, an RNN layer (which may itself have multiple layers) that operates on that embedded state and a hidden state, and a decoder layer that outputs the predicted character scores distribution.\n",
    "\n",
    "\n",
    "You must implement your model in the `rnn/model.py` file. You should use a `nn.Embedding` object for the encoding layer, a RNN model like `nn.RNN` or `nn.LSTM`, and a `nn.Linear` layer for the final a predicted character score decoding layer.\n",
    "\n",
    "\n",
    "**TODO:** Implement the model in RNN `rnn/model.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g4R6tTYiKa1t"
   },
   "source": [
    "# Evaluating\n",
    "\n",
    "To evaluate the network we will feed one character at a time, use the outputs of the network as a probability distribution for the next character, and repeat. To start generation we pass a priming string to start building up the hidden state, from which we then generate one character at a time.\n",
    "\n",
    "\n",
    "Note that in the `evaluate` function, every time a prediction is made the outputs are divided by the \"temperature\" argument. Higher temperature values make actions more equally likely giving more \"random\" outputs. Lower temperature values (less than 1) high likelihood options contribute more. A temperature near 0 outputs only the most likely outputs.\n",
    "\n",
    "You may check different temperature values yourself, but we have provided a default which should work well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ll6tjmE4Ka1t"
   },
   "outputs": [],
   "source": [
    "def evaluate(rnn, prime_str='A', predict_len=100, temperature=0.8):\n",
    "    hidden = rnn.init_hidden(1, device=device)\n",
    "    prime_input = char_tensor(prime_str)\n",
    "    predicted = prime_str\n",
    "\n",
    "    # Use priming string to \"build up\" hidden state\n",
    "    for p in range(len(prime_str) - 1):\n",
    "        _, hidden = rnn(prime_input[p].unsqueeze(0).to(device), hidden)\n",
    "    inp = prime_input[-1]\n",
    "    \n",
    "    for p in range(predict_len):\n",
    "        output, hidden = rnn(inp.unsqueeze(0).to(device), hidden)\n",
    "        \n",
    "        # Sample from the network as a multinomial distribution\n",
    "        output_dist = output.data.view(-1).div(temperature).exp()\n",
    "        top_i = torch.multinomial(output_dist, 1)[0]\n",
    "        \n",
    "        # Add predicted character to string and use as next input\n",
    "        predicted_char = all_characters[top_i]\n",
    "        predicted += predicted_char\n",
    "        inp = char_tensor(predicted_char)\n",
    "\n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5hOjLcnTKa1t"
   },
   "source": [
    "# Train RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jrjWaJxTKa1t"
   },
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "n_epochs = 5000\n",
    "hidden_size = 150\n",
    "n_layers = 4\n",
    "learning_rate = 0.001\n",
    "model_type = 'gru'\n",
    "print_every = 50\n",
    "plot_every = 50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vg-XC17_Ka1t"
   },
   "outputs": [],
   "source": [
    "def eval_test(rnn, inp, target):\n",
    "    with torch.no_grad():\n",
    "        hidden = rnn.init_hidden(batch_size, device=device)\n",
    "        loss = 0\n",
    "        for c in range(chunk_len):\n",
    "            output, hidden = rnn(inp[:,c], hidden)\n",
    "            loss += criterion(output.view(batch_size, -1), target[:,c])\n",
    "    \n",
    "    return loss.data.item() / chunk_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A1PQEbkUKa1t"
   },
   "source": [
    "### Train function\n",
    "\n",
    "**TODO**: Fill in the train function. You should initialize a hidden layer representation using your RNN's `init_hidden` function, set the model gradients to zero, and loop over each time step (character) in the input tensor. For each time step compute the output of the of the RNN and compute the loss over the output and the corresponding ground truth time step in `target`. The loss should be averaged over all time steps. Lastly, call backward on the averaged loss and take an optimizer step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q5v7zqz9Ka1t"
   },
   "outputs": [],
   "source": [
    "def train(rnn, input, target, optimizer, criterion):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    - rnn: model\n",
    "    - input: input character data tensor of shape (batch_size, chunk_len)\n",
    "    - target: target character data tensor of shape (batch_size, chunk_len)\n",
    "    - optimizer: rnn model optimizer\n",
    "    - criterion: loss function\n",
    "    \n",
    "    Returns:\n",
    "    - loss: computed loss value as python float\n",
    "    \"\"\"\n",
    "    loss = 0\n",
    "    \n",
    "    ####################################\n",
    "    #          YOUR CODE HERE          #\n",
    "    ####################################\n",
    "    batch_size = input.shape[0]\n",
    "    hidden = rnn.init_hidden(batch_size, device)\n",
    "    rnn.zero_grad()\n",
    "\n",
    "    input = input.to(device)\n",
    "    target = target.to(device)\n",
    "\n",
    "    for c in range(chunk_len):\n",
    "        output, hidden = rnn(input[:, c], hidden)\n",
    "        loss += criterion(output, target[:, c])\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    ##########       END      ##########\n",
    "\n",
    "    return loss.data.item() / chunk_len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9lZsZ-b7Ka1t",
    "outputId": "38d13aa5-7323-4656-fa8f-804069098464"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 5000 epochs...\n",
      "[0m 45s (50 1%) train loss: 3.3300, test_loss: 3.3086]\n",
      "Whdh :eo snog wr h\n",
      "i:ase ee a oeessth ml wet \n",
      "it dtyto aar o \n",
      "\n",
      "oOres  \n",
      "ah ehUt \n",
      " ea\n",
      "ioarea shth \n",
      "'isth \n",
      "\n",
      "[1m 30s (100 2%) train loss: 3.2461, test_loss: 3.2649]\n",
      "Whcou e  oonk\n",
      "O\n",
      "i  ldn  el i\n",
      "ts  dn od\n",
      " tret lhi seI  huae\n",
      " iy pi 'w orfuAHn  t ho,d atw tesS ell asat \n",
      "\n",
      "[2m 16s (150 3%) train loss: 2.7817, test_loss: 2.7605]\n",
      "Whs uers oahe are, lhey su nrolusse od mt hsiom waanphoe oonrk on ftht yeegt ie eteunWw. uttaaht ter\n",
      "N \n",
      "\n",
      "[3m 2s (200 4%) train loss: 2.6196, test_loss: 2.6162]\n",
      "Whtmelaie hsll! wve aosrds dentd oe ahe ere mfln'd sitill id bo, eor tlctrlet lo powrh eo to id tpaotk \n",
      "\n",
      "[3m 47s (250 5%) train loss: 2.3921, test_loss: 2.4057]\n",
      "Whoed het we to ool,\n",
      "Worfalul celes pon tirm to to nor,\n",
      "Arary to so ose'd, onluud alar.\n",
      "\n",
      "FUOU:\n",
      "Those f \n",
      "\n",
      "[4m 32s (300 6%) train loss: 2.2360, test_loss: 2.2578]\n",
      "Whiuns.\n",
      "Bake:\n",
      "For doth me pelt tharss ussend the I helt thit'mes go be lold tto pless opse as rele so  \n",
      "\n",
      "[5m 18s (350 7%) train loss: 2.1397, test_loss: 2.1434]\n",
      "Whor:\n",
      "\n",
      "UONTOCAN:\n",
      "Mine thall:\n",
      "Het nur fabent a down love leul my wouve old ose the this, fone irarded?\n",
      " \n",
      "\n",
      "[6m 4s (400 8%) train loss: 2.0328, test_loss: 2.0620]\n",
      "Whake who; I with mansiteve,\n",
      "Unto my the the the helled,\n",
      "Or fould the my pleadind hin as yould conder  \n",
      "\n",
      "[6m 49s (450 9%) train loss: 1.9788, test_loss: 1.9879]\n",
      "What an be theer honess doy:\n",
      "I well is his the cower a wats, all the aor enturt.\n",
      "\n",
      "DORGY HILUS:\n",
      "The hou \n",
      "\n",
      "[7m 35s (500 10%) train loss: 1.9070, test_loss: 1.9067]\n",
      "What entain that this guetfein save gow in the put\n",
      "for heived imerth of hithen of as not her thy my gr \n",
      "\n",
      "[8m 22s (550 11%) train loss: 1.8159, test_loss: 1.8366]\n",
      "What there me it to iver but\n",
      "I wiccat the kepon in cheech. Whing of spemited rey and nome in to dpatil \n",
      "\n",
      "[9m 10s (600 12%) train loss: 1.7797, test_loss: 1.8277]\n",
      "What in the sing port mesters,\n",
      "When second your grown thee dickoness and then's fair!\n",
      "\n",
      "DONNTFISZEL:\n",
      "I  \n",
      "\n",
      "[9m 57s (650 13%) train loss: 1.7303, test_loss: 1.7734]\n",
      "Where. That is have sprokes!\n",
      "O thenell but is here with theins\n",
      "And all secheth is please me to a capte \n",
      "\n",
      "[10m 44s (700 14%) train loss: 1.7253, test_loss: 1.7390]\n",
      "What\n",
      "Let by thou have creasic, brow us witkes, I masting,\n",
      "Klound--O Hakes a of so as incellent.\n",
      "\n",
      "FARFR \n",
      "\n",
      "[11m 32s (750 15%) train loss: 1.6608, test_loss: 1.7354]\n",
      "What a shall tell you!\n",
      "\n",
      "LARCENTIO:\n",
      "Mereater, adught so, enere seeflot to this,t\n",
      "Of trown of short is b \n",
      "\n",
      "[12m 20s (800 16%) train loss: 1.6491, test_loss: 1.6827]\n",
      "What so that fayerd, of I pentence the clow\n",
      "In the bo of the the provises and so,\n",
      "And make it om the d \n",
      "\n",
      "[13m 7s (850 17%) train loss: 1.6219, test_loss: 1.6870]\n",
      "What comples my lell!\n",
      "\n",
      "COLONIO:\n",
      "The storiber'd, and with better: and marry!\n",
      "When my letter that mean f \n",
      "\n",
      "[13m 54s (900 18%) train loss: 1.6407, test_loss: 1.6815]\n",
      "What the mack my cannot fallal\n",
      "That not some a sepered ucents,\n",
      "This be demax.\n",
      "This mergarety tongue of \n",
      "\n",
      "[14m 41s (950 19%) train loss: 1.5795, test_loss: 1.6421]\n",
      "What see of it love of your tears his done.\n",
      "I have you noble as the distal-eceike.\n",
      "\n",
      "SLALIL:\n",
      "I may fath \n",
      "\n",
      "[15m 28s (1000 20%) train loss: 1.5710, test_loss: 1.6341]\n",
      "Whus; as a painter, and thou seel\n",
      "My count that day of sorrow, but his perarl\n",
      "Have is my lord him and  \n",
      "\n",
      "[16m 16s (1050 21%) train loss: 1.5665, test_loss: 1.6180]\n",
      "What is that door?\n",
      "\n",
      "ALANENIO:\n",
      "But look me with duke with decoon it.\n",
      "\n",
      "BRUGUNU:\n",
      "Ha Mark with a good cont \n",
      "\n",
      "[17m 3s (1100 22%) train loss: 1.4919, test_loss: 1.6062]\n",
      "What is world,\n",
      "This wind you come again a hours Fiegred\n",
      "Wherefer God that say him to me. Come, whyn,\n",
      "I \n",
      "\n",
      "[17m 50s (1150 23%) train loss: 1.5232, test_loss: 1.5924]\n",
      "Whor the\n",
      "a man to mine were's a commons, and the soul, if\n",
      "when so of the man and all the protest: ip m \n",
      "\n",
      "[18m 37s (1200 24%) train loss: 1.4874, test_loss: 1.6065]\n",
      "What cally, if her in thy enemies.\n",
      "\n",
      "BETISH:\n",
      "To lare, being to him.\n",
      "\n",
      "LEONTAS:\n",
      "This come of\n",
      "'Tis not tha \n",
      "\n",
      "[19m 25s (1250 25%) train loss: 1.4678, test_loss: 1.5694]\n",
      "What diess bear the over-ather,--\n",
      "\n",
      "OGER:\n",
      "That we with your hands, likest this onier of do.\n",
      "\n",
      "KING HENRY \n",
      "\n",
      "[20m 12s (1300 26%) train loss: 1.5043, test_loss: 1.5676]\n",
      "Where in the fal consent.\n",
      "Joose the full and much a centreness,\n",
      "The world heard thee their wit it cous \n",
      "\n",
      "[20m 59s (1350 27%) train loss: 1.4879, test_loss: 1.5834]\n",
      "Wherein you so slame and sheep'd with this tempen day.\n",
      "And shall keep thee. And to their fancies, tell \n",
      "\n",
      "[21m 47s (1400 28%) train loss: 1.4530, test_loss: 1.5464]\n",
      "Where do reason of the should understame.\n",
      "What thou bogness which all that you shall be high whom\n",
      "a fa \n",
      "\n",
      "[22m 34s (1450 28%) train loss: 1.4604, test_loss: 1.5484]\n",
      "What not place him but to entreat\n",
      "The come that my name: he better their tongue,\n",
      "In the much a walk in \n",
      "\n",
      "[23m 22s (1500 30%) train loss: 1.4303, test_loss: 1.5371]\n",
      "What sheir intent into the\n",
      "Duke in time with the world to this boson,\n",
      "inderstand with most time and to \n",
      "\n",
      "[24m 10s (1550 31%) train loss: 1.4394, test_loss: 1.5517]\n",
      "What shall make the messence,\n",
      "And thus, I must stay under Antoner:\n",
      "Then there is the meant that Rome c \n",
      "\n",
      "[24m 57s (1600 32%) train loss: 1.4246, test_loss: 1.5277]\n",
      "Whore you my sunner-time.\n",
      "\n",
      "KING HENRY V:\n",
      "God man with arm as read's more thing it slain\n",
      "Sow of my hors \n",
      "\n",
      "[25m 45s (1650 33%) train loss: 1.4212, test_loss: 1.5374]\n",
      "Where are so;\n",
      "She will umlowly is his at his arse,\n",
      "And speak in my lord, to the heart for him;\n",
      "And yet \n",
      "\n",
      "[26m 32s (1700 34%) train loss: 1.4223, test_loss: 1.5441]\n",
      "What is power bare an oathed hath\n",
      "Lend the tongue from this mirms have he that\n",
      "Deeping as strong shall \n",
      "\n",
      "[27m 19s (1750 35%) train loss: 1.4313, test_loss: 1.4942]\n",
      "Who and for ever stumble me to you.\n",
      "\n",
      "LUCIO:\n",
      "Wherefore, thou hadst thou virtued me.\n",
      "\n",
      "DUKE SENION:\n",
      "That  \n",
      "\n",
      "[28m 7s (1800 36%) train loss: 1.3875, test_loss: 1.5333]\n",
      "What should not now on the bolding,\n",
      "And by mine eyes and sends and mensel to\n",
      "Field you not with must b \n",
      "\n",
      "[28m 55s (1850 37%) train loss: 1.3809, test_loss: 1.5419]\n",
      "When the received to her fate.\n",
      "\n",
      "EDGAR:\n",
      "A vapay elet of Romeo, her knugled;\n",
      "Yet thou sleep, like better \n",
      "\n",
      "[29m 42s (1900 38%) train loss: 1.3956, test_loss: 1.4941]\n",
      "Whether to a daughter saas?\n",
      "Love him most how my sake. Look out of far,\n",
      "O'er I praise her: when I do b \n",
      "\n",
      "[30m 29s (1950 39%) train loss: 1.3828, test_loss: 1.5253]\n",
      "What's not in the king's will not be leave;\n",
      "The people of call him in the morning of dead\n",
      "That in easu \n",
      "\n",
      "[31m 17s (2000 40%) train loss: 1.4112, test_loss: 1.5477]\n",
      "Whank you think we blest the better talents.\n",
      "\n",
      "DESDEMONA:\n",
      "So honour all thy house dissemple.\n",
      "\n",
      "Third Pas \n",
      "\n",
      "[32m 5s (2050 41%) train loss: 1.3680, test_loss: 1.4716]\n",
      "What would be doth does dreave\n",
      "The court to her twenty Raver\n",
      "For the earn than grace at tried and leav \n",
      "\n",
      "[32m 52s (2100 42%) train loss: 1.3836, test_loss: 1.5169]\n",
      "When I say; what is the most subbles and\n",
      "flight.\n",
      "\n",
      "FALSTAFF:\n",
      "I'' stass, you will have desires to bear u \n",
      "\n",
      "[33m 40s (2150 43%) train loss: 1.3733, test_loss: 1.4640]\n",
      "What can confirm?\n",
      "\n",
      "ROCHARLE:\n",
      "I will I did thy tongues with strong and own kind\n",
      "Since our bloody intell \n",
      "\n",
      "[34m 27s (2200 44%) train loss: 1.3477, test_loss: 1.4663]\n",
      "Which in a sub botch of Percy in\n",
      "They have life said lord of chastess, be never heart\n",
      "Should we are th \n",
      "\n",
      "[35m 15s (2250 45%) train loss: 1.3522, test_loss: 1.5099]\n",
      "Why pitied of Sir what thou both, as Luest,\n",
      "Most as much after it.\n",
      "\n",
      "LUCENTIO:\n",
      "Pardon they cannot die h \n",
      "\n",
      "[36m 3s (2300 46%) train loss: 1.3697, test_loss: 1.4791]\n",
      "When will not deam: it was news must do,\n",
      "Some wise to the thing as humour's doom;\n",
      "With thy best offent \n",
      "\n",
      "[36m 50s (2350 47%) train loss: 1.3596, test_loss: 1.4798]\n",
      "When you do not have the worn her husband,\n",
      "In almore figure of carriage well.\n",
      "\n",
      "AEGEON:\n",
      "O, there shall  \n",
      "\n",
      "[37m 37s (2400 48%) train loss: 1.3618, test_loss: 1.4920]\n",
      "What man, have that is not what he sport the prince\n",
      "And nature all my property of Lysander,\n",
      "Which neve \n",
      "\n",
      "[38m 25s (2450 49%) train loss: 1.3537, test_loss: 1.4905]\n",
      "When this merriful do not less were we\n",
      "day of his horse.\n",
      "\n",
      "ANTIPHOLUS OF SYRACUSE:\n",
      "The certain on, he's \n",
      "\n",
      "[39m 13s (2500 50%) train loss: 1.4001, test_loss: 1.4579]\n",
      "Where would see thee.\n",
      "\n",
      "Gentleman:\n",
      "If he see; I did love you such deserved for throws\n",
      "Chief! Sir varian \n",
      "\n",
      "[40m 0s (2550 51%) train loss: 1.3584, test_loss: 1.5012]\n",
      "What father is like the perspient thus;\n",
      "I have repent up and instrument and drops\n",
      "And at a mouth, he t \n",
      "\n",
      "[40m 48s (2600 52%) train loss: 1.3258, test_loss: 1.4937]\n",
      "When be so such like them; there is fear\n",
      "from the death. he's a good poaster; I am part-madam paid\n",
      "hom \n",
      "\n",
      "[41m 36s (2650 53%) train loss: 1.3470, test_loss: 1.4835]\n",
      "What mark me to the fool; and when here\n",
      "That by the matter, the probot lords of father.\n",
      "\n",
      "KING EDWARD I \n",
      "\n",
      "[42m 24s (2700 54%) train loss: 1.3454, test_loss: 1.4793]\n",
      "What, he's my mistress\n",
      "Have booth for a hair: and your father savours.\n",
      "\n",
      "WILLIAM:\n",
      "This hated his talexa \n",
      "\n",
      "[43m 12s (2750 55%) train loss: 1.3382, test_loss: 1.4574]\n",
      "Whanks are sore for living were not devent the matter?\n",
      "\n",
      "GLOUCESTER:\n",
      "That you have been shadow; of his  \n",
      "\n",
      "[44m 0s (2800 56%) train loss: 1.3403, test_loss: 1.4850]\n",
      "Whose husband and this tribunes to cut the matter\n",
      "horseless come another than he said in the Nestant m \n",
      "\n",
      "[44m 48s (2850 56%) train loss: 1.3293, test_loss: 1.4602]\n",
      "When I have but this Moor mercy, and scarce\n",
      "Of what this be thee all trowl thereee wherewife.\n",
      "\n",
      "CASSIUS \n",
      "\n",
      "[45m 36s (2900 57%) train loss: 1.3219, test_loss: 1.4809]\n",
      "Who here: give you our truth up me: I speak\n",
      "In that high made of his pointry. Therefore I\n",
      "I would my b \n",
      "\n",
      "[46m 24s (2950 59%) train loss: 1.3330, test_loss: 1.4651]\n",
      "What's to the field freely further too;\n",
      "He be gentle as thou hear to it, for whom\n",
      "Though rebel to the  \n",
      "\n",
      "[47m 12s (3000 60%) train loss: 1.3201, test_loss: 1.4573]\n",
      "What sad speak whom he breathes to attend me a man\n",
      "That he would sitriding me thy drown.\n",
      "\n",
      "NORTHUMBERLA \n",
      "\n",
      "[48m 0s (3050 61%) train loss: 1.3171, test_loss: 1.4532]\n",
      "What hath a everlasting worth,\n",
      "And to hear it with him good.\n",
      "\n",
      "Soothing Lucius:\n",
      "Herein he trial to know \n",
      "\n",
      "[48m 49s (3100 62%) train loss: 1.3539, test_loss: 1.4791]\n",
      "Where I wouldst my love as the city,\n",
      "As bride his particulate take the worth of Rome.\n",
      "This kind of her \n",
      "\n",
      "[49m 37s (3150 63%) train loss: 1.3219, test_loss: 1.4625]\n",
      "When you are full speak of my state, think shall wever\n",
      "Is it from a lady, and your crafter ear,\n",
      "And so \n",
      "\n",
      "[50m 25s (3200 64%) train loss: 1.3211, test_loss: 1.4797]\n",
      "Why the better more hands a mortal stroke.\n",
      "\n",
      "OTHELLO:\n",
      "Marsher, who that\n",
      "Looks it down the worst of figh \n",
      "\n",
      "[51m 13s (3250 65%) train loss: 1.3112, test_loss: 1.4523]\n",
      "What grows I fight of the common in this passion,\n",
      "And command you, mave not me here:\n",
      "And make the comm \n",
      "\n",
      "[52m 1s (3300 66%) train loss: 1.3287, test_loss: 1.4439]\n",
      "When for them, naught on I rascal.\n",
      "\n",
      "BERTRAM:\n",
      "Let me be the city had shook at good weak\n",
      "The voice, thus \n",
      "\n",
      "[52m 49s (3350 67%) train loss: 1.2946, test_loss: 1.4633]\n",
      "What winds the forth in should to find thy conscience;\n",
      "Strike the rest tongue is fan wife, at once no  \n",
      "\n",
      "[53m 37s (3400 68%) train loss: 1.3072, test_loss: 1.4590]\n",
      "What should feacheth him?\n",
      "\n",
      "DEMETRIUS:\n",
      "And he was and stolen his mean with truth.\n",
      "\n",
      "ENGES:\n",
      "Sir, I'll be  \n",
      "\n",
      "[54m 25s (3450 69%) train loss: 1.3010, test_loss: 1.4469]\n",
      "What came of Honicure?\n",
      "\n",
      "NORTHUMBERLAND:\n",
      "A countered living heaven mockus,\n",
      "And the spirits, upon the co \n",
      "\n",
      "[55m 13s (3500 70%) train loss: 1.3007, test_loss: 1.4424]\n",
      "Where's here were not in our soldier:\n",
      "With obtaining of his own griefs, be best piece\n",
      "My living in: th \n",
      "\n",
      "[56m 2s (3550 71%) train loss: 1.3242, test_loss: 1.4734]\n",
      "Whou art a gentleman to court of grace, a rights\n",
      "That this devils shall I were her others women.\n",
      "What  \n",
      "\n",
      "[56m 50s (3600 72%) train loss: 1.3075, test_loss: 1.4641]\n",
      "What most be a thing arms?\n",
      "\n",
      "SLENDER:\n",
      "Indeed so,\n",
      "that, I will so hot persuade the\n",
      " less the death broth \n",
      "\n",
      "[57m 38s (3650 73%) train loss: 1.3053, test_loss: 1.4406]\n",
      "Whith since the father things for strength of mine,\n",
      "While you shall give them above his recowan!\n",
      "\n",
      "FLUE \n",
      "\n",
      "[58m 26s (3700 74%) train loss: 1.3117, test_loss: 1.4614]\n",
      "What you shall be stir?\n",
      "\n",
      "KING HENRY V:\n",
      "More filght thou be and mine own nigng danger.\n",
      "\n",
      "HOLOFERNES:\n",
      "Wel \n",
      "\n",
      "[59m 14s (3750 75%) train loss: 1.2989, test_loss: 1.4370]\n",
      "What is thy soul upon the day shall think\n",
      "power, but I was a good save the dukedom with me\n",
      "and in to t \n",
      "\n",
      "[60m 2s (3800 76%) train loss: 1.2989, test_loss: 1.4207]\n",
      "What you that strike this experity,\n",
      "And so much is this lial shows to the word\n",
      "Again what cousin Westm \n",
      "\n",
      "[60m 50s (3850 77%) train loss: 1.2779, test_loss: 1.4636]\n",
      "When remembrance of such ivery thing at the confirm.\n",
      "\n",
      "STANLEY:\n",
      "No, how makes me sir!\n",
      "\n",
      "SURREY:\n",
      "Go of yo \n",
      "\n",
      "[61m 37s (3900 78%) train loss: 1.2909, test_loss: 1.4203]\n",
      "What should not more worse with matter\n",
      "Of Sir bottle and displeater Captain,\n",
      "Let her be a try to my to \n",
      "\n",
      "[62m 25s (3950 79%) train loss: 1.3138, test_loss: 1.4291]\n",
      "When I'll do you give a quiet;\n",
      "Them fell home's by your hate, and that suit\n",
      "As strange that, at that,  \n",
      "\n",
      "[63m 13s (4000 80%) train loss: 1.2961, test_loss: 1.4380]\n",
      "What would be topples in the lie? I\n",
      "rave thy words?\n",
      "\n",
      "Second Servand:\n",
      "Good great you to the seasure: th \n",
      "\n",
      "[64m 0s (4050 81%) train loss: 1.2757, test_loss: 1.4666]\n",
      "What shield have you done?\n",
      "\n",
      "PANDARUS:\n",
      "I was recount me again; so weak it. Look is an extent:\n",
      "I think n \n",
      "\n",
      "[64m 48s (4100 82%) train loss: 1.3200, test_loss: 1.4234]\n",
      "Why harm with her with those that they proceeded\n",
      "The unhappy bent to the duke that starrence\n",
      "Between t \n",
      "\n",
      "[65m 36s (4150 83%) train loss: 1.2931, test_loss: 1.4588]\n",
      "Who you are as much oaths, the since is another,\n",
      "if every money had the reason of the old, and\n",
      "there i \n",
      "\n",
      "[66m 23s (4200 84%) train loss: 1.2837, test_loss: 1.4264]\n",
      "Why that will, that will their wonder\n",
      "That shall be mine in these strength and sickness\n",
      "Are sent and b \n",
      "\n",
      "[67m 11s (4250 85%) train loss: 1.3262, test_loss: 1.4519]\n",
      "What have thou art none, or arm?\n",
      "\n",
      "HAMLET:\n",
      "Thou art to dream to my choice.\n",
      "\n",
      "COSTARD:\n",
      "This is that I pra \n",
      "\n",
      "[67m 59s (4300 86%) train loss: 1.3065, test_loss: 1.4178]\n",
      "Where's it would see it in the bush unto\n",
      "thee; and, I was sent out to his good will not know,\n",
      "where is \n",
      "\n",
      "[68m 46s (4350 87%) train loss: 1.2940, test_loss: 1.4460]\n",
      "What's the waded men was a torture in your grace?\n",
      "\n",
      "MARCUS ANDRONICUS:\n",
      "True, my lord, and these man at  \n",
      "\n",
      "[69m 34s (4400 88%) train loss: 1.2873, test_loss: 1.4266]\n",
      "What support\n",
      "In she that with his tongues do invented:\n",
      "And, lest your happy complexion\n",
      "It show me what \n",
      "\n",
      "[70m 22s (4450 89%) train loss: 1.2740, test_loss: 1.4201]\n",
      "Where policy the lands be\n",
      "Where such a plantagenet of thicken men,\n",
      "There's sore speak when pour night  \n",
      "\n",
      "[71m 10s (4500 90%) train loss: 1.2875, test_loss: 1.4178]\n",
      "What we send his help of the sea! I must not bear his valiant\n",
      "of thy lord.\n",
      "You come not well again to  \n",
      "\n",
      "[71m 57s (4550 91%) train loss: 1.2734, test_loss: 1.4016]\n",
      "What are we be nease o' the tread him?\n",
      "\n",
      "DESDEMONA:\n",
      "I can say no marriage.\n",
      "\n",
      "BAPTISTA:\n",
      "Well, my lord, I  \n",
      "\n",
      "[72m 45s (4600 92%) train loss: 1.2688, test_loss: 1.4417]\n",
      "What, a deed, O, beseech you,\n",
      "Even to believe him arrived so,\n",
      "Your majesty service are as strokes:\n",
      "I w \n",
      "\n",
      "[73m 32s (4650 93%) train loss: 1.2820, test_loss: 1.4192]\n",
      "Why new standing gives the Welshman\n",
      "As we have they rock repeal at it.\n",
      "\n",
      "PANDARUS:\n",
      "But that art thou sa \n",
      "\n",
      "[74m 20s (4700 94%) train loss: 1.2797, test_loss: 1.4501]\n",
      "Why tears do to the testament of thine own shore\n",
      "Will through a prisoners as anon, our time,\n",
      "To have t \n",
      "\n",
      "[75m 8s (4750 95%) train loss: 1.2750, test_loss: 1.4466]\n",
      "When he had signior themselves\n",
      "With way and her father's brother's part\n",
      "As wicked armed through of moc \n",
      "\n",
      "[75m 55s (4800 96%) train loss: 1.2765, test_loss: 1.4228]\n",
      "What tender mettle? O you shall be sad?\n",
      "\n",
      "FRANCIS:\n",
      "If there is no much upon thy face; for all\n",
      "convenien \n",
      "\n",
      "[76m 43s (4850 97%) train loss: 1.2624, test_loss: 1.4099]\n",
      "What do not take him their soul there?\n",
      "\n",
      "FERDINAND:\n",
      "I with good lord, I fear your pity.\n",
      "\n",
      "SPEED:\n",
      "I thank \n",
      "\n",
      "[77m 31s (4900 98%) train loss: 1.2777, test_loss: 1.4450]\n",
      "Why in his own bonato entreat\n",
      "Should be in the guiltless month to Clarence.\n",
      "\n",
      "First Senator:\n",
      "The traito \n",
      "\n",
      "[78m 18s (4950 99%) train loss: 1.2749, test_loss: 1.4590]\n",
      "What, give me with forest, who back as it is for\n",
      "flawer out to both and devotion is well more\n",
      "men pres \n",
      "\n",
      "[79m 6s (5000 100%) train loss: 1.2609, test_loss: 1.4315]\n",
      "Who will I make a work so much\n",
      "love that which my house, thou hast with no more of\n",
      "with Troy! I will b \n",
      "\n"
     ]
    }
   ],
   "source": [
    "rnn = RNN(n_characters, hidden_size, n_characters, model_type=model_type, n_layers=n_layers).to(device)\n",
    "rnn_optimizer = torch.optim.Adam(rnn.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "start = time.time()\n",
    "all_losses = []\n",
    "test_losses = []\n",
    "loss_avg = 0\n",
    "test_loss_avg = 0\n",
    "\n",
    "\n",
    "print(\"Training for %d epochs...\" % n_epochs)\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    loss = train(rnn, *load_random_batch(train_text, chunk_len, batch_size), rnn_optimizer, criterion)\n",
    "    loss_avg += loss\n",
    "    \n",
    "    test_loss = eval_test(rnn, *load_random_batch(test_text, chunk_len, batch_size))\n",
    "    test_loss_avg += test_loss\n",
    "\n",
    "    if epoch % print_every == 0:\n",
    "        print('[%s (%d %d%%) train loss: %.4f, test_loss: %.4f]' % (time_since(start), epoch, epoch / n_epochs * 100, loss, test_loss))\n",
    "        print(generate(rnn, 'Wh', 100, device=device), '\\n')\n",
    "\n",
    "    if epoch % plot_every == 0:\n",
    "        all_losses.append(loss_avg / plot_every)\n",
    "        test_losses.append(test_loss_avg / plot_every)\n",
    "        loss_avg = 0\n",
    "        test_loss_avg = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zPWNdBZLKa1t"
   },
   "outputs": [],
   "source": [
    "# save network\n",
    "torch.save(rnn.state_dict(), './rnn_generator.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BDOMQpduKa1t"
   },
   "source": [
    "# Plot the Training and Test Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "aihDjrJ_Ka1t",
    "outputId": "2f440ffc-1112-4fde-d5f7-29780576fbd5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fd543421630>]"
      ]
     },
     "execution_count": 19,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(all_losses)\n",
    "plt.plot(test_losses, color='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wIr8aQ2LKa1t"
   },
   "source": [
    "# Evaluate text generation\n",
    "\n",
    "Check what the outputted text looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "je1B-mvqKa1t",
    "outputId": "0c60e2ef-ebf3-4598-aad9-601b8dacb90d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Then call his party.\n",
      "Oft any man I am fairest for arpent, and bear\n",
      "Are pass. Now, call me to the detenty.\n",
      "\n",
      "AEGEON:\n",
      "I will not put on.\n",
      "\n",
      "LEONATO:\n",
      "Madam, combat, yet he had never can so strike him\n",
      "As cheekl to a face to be forth two want.\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "What wrong soon,\n",
      "From the sight, that you should fly to Ford under a\n",
      "You choose point, and in the sea to liminius,\n",
      "And so he is none and his babe Cains:\n",
      "Of heavens and made back and heart to fear\n",
      "The heart of what he has spleave here as it\n",
      "Which oft she presently to thine eyes.\n",
      "\n",
      "LPAVIA:\n",
      "I'll thy error to early eye on her father.\n",
      "\n",
      "BENVOLIO:\n",
      "No, the wrongs of grace that we perchance with a\n",
      "she is. What says Looked poyer, I will be so, that were\n",
      "the self, and Timon! may I find up your grace?\n",
      "\n",
      "BAPTISTA:\n",
      "Ay, therein by safety, and let them come\n",
      "Or holy death wide-royal looks and Alic,\n",
      "Farewell to the Thracious untimely virtue,\n",
      "What we have, and go to the pound for with his grave\n",
      "And Naence men well, and I am in Rome.\n",
      "\n",
      "KING HENRY VIII:\n",
      "Then he w\n"
     ]
    }
   ],
   "source": [
    "print(evaluate(rnn, prime_str='Th', predict_len=1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L-qRbod0Ka1t"
   },
   "source": [
    "# Hyperparameter Tuning\n",
    "\n",
    "Some things you should try to improve your network performance are:\n",
    "- Different RNN types. Switch the basic RNN network in your model to a GRU and LSTM to compare all three.\n",
    "- Try adding 1 or two more layers\n",
    "- Increase the hidden layer size\n",
    "- Changing the learning rate\n",
    "\n",
    "**TODO:** Try changing the RNN type and hyperparameters. Record your results."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "name": "MP4_P2_generation.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
